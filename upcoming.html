<!DOCTYPE>
<html>
  Some obvious and some maybe less obvious musings:

Capital is allocated, money is transacted. Capital is productive, money is stagnant. Liquidity and productivity are perhaps inversely correlated (though I suppose event contracts contradicts this rule). I’ve noticed the more edge an asset has, the less liquid it will probably be. So risk and liquidity are also tied together which is why the biggest ROI plays come from the most illiquid assets. Hence venture capital. Probably obvious to most players but I thought it was important to formulate. 

I also think as of right now (December 2024) that LLMs are not going to be the general basis for ASI. My thesis on LLMs currently is that the next token prediction is obviously the correct approach to natural language (even as I am writing this, it feels like I am just predicting the next token based on the first word of this sentence). Ever notice a human blindly choose some opinion, and just spam generate text in support of the position? That, IMO, is the biggest argument in support of next token prediction as proper human modeling. However, looking at the problem as a whole rationally, there is so much more work to be done in planning, synthesis, reinforcement and more. LLMs are colloquially called foundation models right now, but I posit that what will come is a true synthesis of all intelligent tasks in one actually foundational model. Perhaps the model should run cortically, as in instead of using all the weights, it just uses cortex or different parts of the weights imitating brain cortexes. I have no idea if this has been tried. Maybe it should be. 

An interesting steelman that I just came up with for the argument that LLMs are ASI, is that language as we know it is uniquely human, so it’s possible that developing super-language is what will become super intelligence. But I am not so convinced of this, regardless. It seems intuitive to me that language cannot simply be the end all be all. There has to be some algorithmic components to intelligence, even in an NN model. Our brains are NNs but we have learned algorithms, such as addition, that don’t require some sort of stochastic next token prediction. LLMs should be able to learn actual algorithms. When humans see a + sign, we don’t think to still engage in next token prediction and just try to guess at an answer. So in a similar way, there needs to be a modification to transformer architecture that allows it to physically learn functional representations. 
